{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-hPgFVZH3ao"
   },
   "source": [
    "**What we are going to output**\n",
    "1. Is cancer or Not cancer\n",
    "2. Which type of cancer (bcc, mel) or non-cancer (akiec, bkl, df, nv, vasc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsn9mdaV7GW8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A72HSm9IOVJZ"
   },
   "source": [
    "\n",
    "M(1) ---> P(Cancer)\n",
    "\n",
    "\n",
    "M(2) ---> Max ( P(cancer_type_1),  P(cancertype_2) ... P(cancertype_n) )\n",
    "\n",
    "\n",
    "M(3) ---> Max ( P(benign_type_1),  P(benign_type_2) ... P(benign_type_n) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fi67fasl1Bb_",
    "outputId": "57dae9d4-fa2c-458e-97d7-26ae779be18e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'EECS_442_Project'\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "print(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "# Change into project drive directory.\n",
    "%cd /content/drive/MyDrive/EECS_442_Project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p06Aaib5-Wo8"
   },
   "source": [
    "Checks which device we are using.\n",
    "\n",
    "Note: This code has been pulled from Homework 5: hw5_diffusion.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zofjYa_3-Vy7",
    "outputId": "60fe6f01-ea85-481f-e7fe-1e459d7e674e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPUs are preferred\n",
    "device_str = f\"cuda:0\" if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Current device: \", device_str)\n",
    "device = torch.device(device_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELd4xQbVDPJU"
   },
   "source": [
    "### Compress Images (Preprocessing) - DONT RUN!\n",
    "Converts images from: /content/drive/MyDrive/EECS_442_Project/data/images/\n",
    "\n",
    "This function does the following:\n",
    "- For each image, resive the 600x450 image to 300x300\n",
    "- Set the quality to 90\n",
    "- Reduces image size from 180KB to 11KB with minimal loss in program accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCzW9jRTD4bz"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# directory for the original images and the new directory for the reduced images\n",
    "original_images_path = '/content/drive/MyDrive/EECS_442_Project/data/images/'\n",
    "reduced_images_dir = '/content/drive/MyDrive/EECS_442_Project/data/images_compressed/'\n",
    "\n",
    "# Create the directory for reduced images if it does not exist\n",
    "os.makedirs(reduced_images_dir, exist_ok=True)\n",
    "\n",
    "# Define the desired size and quality\n",
    "desired_size = (300, 300)\n",
    "quality_setting = 90\n",
    "\n",
    "# Function to process images\n",
    "def process_images(image_directory, output_directory, size, quality):\n",
    "    # Iterate over all the files in the original image directory\n",
    "    for filename in os.listdir(image_directory):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(original_images_path)\n",
    "            try:\n",
    "                # Open the image\n",
    "                image = Image.open(file_path)\n",
    "\n",
    "                # Resize the image\n",
    "                image_resized = image.resize(size)\n",
    "\n",
    "                # Construct the output file path\n",
    "                output_file_path = os.path.join(output_directory, 'out-'+str(size)+'-'+str(quality)+'.jpg')\n",
    "\n",
    "                # Save the image with optimization and the set quality\n",
    "                image_resized.save(output_file_path, optimize=True, quality=quality)\n",
    "\n",
    "                print(f'Processed {output_file_path}')\n",
    "            except Exception as e:\n",
    "                print(f'Error processing {original_images_path}: {e}')\n",
    "\n",
    "# Call the function\n",
    "#process_images(original_images_path, reduced_images_dir, desired_size, quality_setting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCG3EbjXTyxP",
    "outputId": "3f7422cf-dafc-440c-d1cd-d614dab11fc0"
   },
   "outputs": [],
   "source": [
    "#image_files = [f for f in os.listdir(reduced_images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "# print(len(image_files))\n",
    "# import os\n",
    "\n",
    "def get_dir_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Replace 'reduced_images_dir' with the actual path of your directory\n",
    "# total_size = get_dir_size(original_images_path)\n",
    "# print(f\"The total size of the folder is: {total_size} bytes\")\n",
    "# print(f\"The total size of the folder is: {total_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSGDrKbsILoY"
   },
   "source": [
    "### ImageData Class\n",
    "Converts images from: /content/drive/MyDrive/EECS_442_Project/data/images/ to Tensors.\n",
    "\n",
    "This class stores the following:\n",
    "- Image_id/name Ex. 'ISIC_0030208'\n",
    "- Classification Ex. 'bkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NjbMEdq4_Zk"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "class ImageData(Dataset):\n",
    "  def __init__(self, preprocess=False, dataset_type='complete', training_type='training'):\n",
    "    self.preprocess = preprocess\n",
    "    self.root = 'data/images/'\n",
    "    self.cancerous_path = 'data/cancerous_HAM10000_metadata.csv'\n",
    "    self.non_cancerous_path = 'data/non_cancerous_HAM10000_metadata.csv'\n",
    "    self.complete_path = 'data/partitioned_HAM10000_metadata.csv'\n",
    "#     self.complete_path = 'data/evenmore_downsampled_partitioned.csv'\n",
    "    self.transform = transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Resize((224, 224))\n",
    "      ])\n",
    "\n",
    "    path = None\n",
    "    if dataset_type == 'complete':\n",
    "      path = self.complete_path\n",
    "    elif dataset_type == 'cancerous':\n",
    "      path = self.cancerous_path\n",
    "    elif dataset_type == 'non_cancerous':\n",
    "      path = self.non_cancerous_path\n",
    "    else:\n",
    "      print(\"Please define path as: complete, cancerous, or non_cancerous\")\n",
    "\n",
    "    self.image_id_list = []\n",
    "    # Initialize a dictionary of metadata.\n",
    "    self.image_metadata_dict = {}\n",
    "    label_dict = {\n",
    "        'training': 'validation',\n",
    "        'validation': 'test',\n",
    "        'test': ''\n",
    "    }\n",
    "\n",
    "\n",
    "    found_type = False\n",
    "    with open(path, mode='r') as file:\n",
    "      metadata = csv.reader(file)\n",
    "      for row in metadata:\n",
    "        if row[0] == training_type:\n",
    "          found_type = True\n",
    "          continue\n",
    "        elif not found_type:\n",
    "          continue\n",
    "\n",
    "        if row[0] == label_dict[training_type]:\n",
    "          break\n",
    "\n",
    "        found_type = True\n",
    "        image_id = row[1]\n",
    "        classification = row[2] # Corresponds to the dx column in the metadata csv\n",
    "        self.image_metadata_dict[image_id] = classification\n",
    "        self.image_id_list.append(image_id)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_metadata_dict)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image_path = self.root + self.image_id_list[idx] + \".jpg\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = self.transform(image)\n",
    "    # print(self.preprocess)\n",
    "    if self.preprocess:\n",
    "      # Calculate image mean and normalize here.\n",
    "      mean = torch.mean(image, dim=(1, 2))\n",
    "      std = torch.std(image, dim=(1, 2))\n",
    "      preprocess_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=(-30, 30)),\n",
    "#         transforms.Normalize(mean=mean, std=std),\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1))\n",
    "      ])\n",
    "      image = preprocess_transform(image)\n",
    "    classification = self.image_metadata_dict[self.image_id_list[idx]]\n",
    "    image_id = self.image_id_list[idx]\n",
    "    return image_id, classification, image.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "espbgjMn9WaO"
   },
   "source": [
    "### DataLoader\n",
    "Creates a dataloader for our ImageData dataset class.\n",
    "\n",
    "Run this to check that data is loading in properly, and that the names and classifications are being correctly associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJrku1bqeS3E"
   },
   "source": [
    "## Guide to using the DataLoader\n",
    "This gives you 3 vectors, each of length batch size.\\\n",
    "`for names, classifications, images in training_data:`\\\n",
    "This loops through the 3 vectors and gives you the specific image as well as the name and classification corresponding to that image.\\\n",
    "`for name, classification, image in zip(names, classifications, images):`\n",
    "\n",
    "\n",
    "There are 3 different data loaders: training_data, validation_data, and test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USSsvk8NQ_nZ"
   },
   "outputs": [],
   "source": [
    "Preprocess = False # True \n",
    "print_preview = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnWhxBBo9WHg"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Tunable Parameters:\n",
    "batch_size = 1\n",
    "shuffle = True\n",
    "\n",
    "training = ImageData(preprocess=Preprocess, dataset_type='complete', training_type='training')\n",
    "training_data = DataLoader(dataset=training, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "validation = ImageData(preprocess=Preprocess, dataset_type='complete', training_type='validation')\n",
    "validation_data = DataLoader(dataset=validation, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# test = ImageData(preprocess=Preprocess, dataset_type='complete', training_type='test')\n",
    "# test_data = DataLoader(dataset=test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "if print_preview:\n",
    "  # Images will be of batch size.\n",
    "  names, classifications, images = next(iter(training_data))\n",
    "  print('Printing training data batch.')\n",
    "  for name, classification, image in zip(names, classifications, images):\n",
    "    print(\"Image name: \", name, \" Image Classification: \", classification)\n",
    "    convert_to_PIL = transforms.ToPILImage()\n",
    "    img = convert_to_PIL(image)\n",
    "    display(img)\n",
    "    print('\\n')\n",
    "\n",
    "  names, classifications, images = next(iter(validation_data))\n",
    "  print('Printing validation data batch.')\n",
    "  for name, classification, image in zip(names, classifications, images):\n",
    "    print(\"Image name: \", name, \" Image Classification: \", classification)\n",
    "    convert_to_PIL = transforms.ToPILImage()\n",
    "    img = convert_to_PIL(image)\n",
    "    display(img)\n",
    "    print('\\n')\n",
    "\n",
    "#   names, classifications, images = next(iter(test_data))\n",
    "#   print('Printing test data batch.')\n",
    "#   for name, classification, image in zip(names, classifications, images):\n",
    "#     print(\"Image name: \", name, \" Image Classification: \", classification)\n",
    "#     convert_to_PIL = transforms.ToPILImage()\n",
    "#     img = convert_to_PIL(image)\n",
    "#     display(img)\n",
    "#     print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fx5zS0XtH2HJ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7qpXWWLIsH2",
    "outputId": "a3961970-c86e-457f-814a-e8447fac90bb"
   },
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXMXX1WII4C6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Displays a progress bar\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcZ9x0FdCr9M"
   },
   "source": [
    "# The First Model (Our design)\n",
    "\n",
    "- Used to detect malignant vs. benign.\n",
    "- Predicts: probability of cell being cancerous\n",
    "- Architecture: 5 convolutional blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGSpEBzNpgBg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FirstModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FirstModel, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "        # Block 1\n",
    "        nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Convolutional layer with 3 input channels (RGB) and 32 output channels\n",
    "        nn.ReLU(),                                    # ReLU activation function to introduce non-linearity\n",
    "        nn.MaxPool2d(2, 2),                           # Max pooling layer to downsample the feature maps\n",
    "        nn.Dropout(0.25),                             # Dropout layer to prevent overfitting by randomly dropping connections\n",
    "\n",
    "        # Block 2\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Convolutional layer with 32 input channels and 64 output channels\n",
    "        nn.ReLU(),                                    # ReLU activation function\n",
    "        nn.MaxPool2d(2, 2),                           # Max pooling\n",
    "        nn.Dropout(0.25),                             # Dropout\n",
    "\n",
    "        # Block 3\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1), # Convolutional layer with 64 input channels and 128 output channels\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "\n",
    "        # Block 4\n",
    "        nn.Conv2d(128, 256, kernel_size=3, padding=1), # Convolutional layer with 128 input channels and 256 output channels\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "\n",
    "        # Block 5\n",
    "        nn.Conv2d(256, 512, kernel_size=3, padding=1), # Convolutional layer with 256 input channels and 512 output channels\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "        # Define the fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 1024),                 # Fully connected layer with input size of 512*7*7 and output size of 1024\n",
    "            nn.ReLU(),                                    # ReLU activation function\n",
    "            nn.Dropout(0.25),                             # Dropout\n",
    "\n",
    "            nn.Linear(1024, 1),                           # Fully connected layer with input size of 1024 and output size of 1\n",
    "            nn.Sigmoid()                                  # Sigmoid activation function to output likelihood of cancer\n",
    "        )\n",
    "#         self.classifier = nn.Sequential(\n",
    "            \n",
    "# #             nn.Linear(512 * 9 * 9, 1024),  # Adjusted the input features\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.25),\n",
    "#             nn.Linear(1024, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional blocks\n",
    "        x = self.features(x)\n",
    "#         print(x.shape)\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass the output through the fully connected layers\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMOV7t5dpmZ9"
   },
   "outputs": [],
   "source": [
    "# training = ImageData(preprocess=Preprocess, dataset_type='complete', training_type='training')\n",
    "# filtered_training_data = DataLoader(dataset=training, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# validation = ImageData(preprocess=Preprocess, dataset_type='complete', training_type='validation')\n",
    "# validation_data = DataLoader(dataset=validation, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "# filtered_training_data = filtered_training_data\n",
    "# validation_data = validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling nc data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mport csv\n",
    "# from torch.utils.data import Dataset\n",
    "# import torchvision.transforms as transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# class ImageData(Dataset):\n",
    "#   def __init__(self, preprocess=False, dataset_type='complete', training_type='training', downsample_nc=False):  # Add the downsample_nc parameter here\n",
    "#     self.preprocess = preprocess\n",
    "#     self.root = 'data/images/'\n",
    "#     self.cancerous_path = 'data/cancerous_HAM10000_metadata.csv'\n",
    "#     self.non_cancerous_path = 'data/non_cancerous_HAM10000_metadata.csv'\n",
    "#     self.complete_path = 'data/partitioned_HAM10000_metadata.csv'\n",
    "#     self.transform = transforms.Compose([\n",
    "#         transforms.PILToTensor(),\n",
    "#         transforms.ConvertImageDtype(torch.float),\n",
    "#         transforms.Resize((224, 224))\n",
    "#     ])\n",
    "\n",
    "#     # Determine the path based on dataset_type\n",
    "#     path = self.complete_path if dataset_type == 'complete' else \\\n",
    "#            self.cancerous_path if dataset_type == 'cancerous' else \\\n",
    "#            self.non_cancerous_path if dataset_type == 'non_cancerous' else \\\n",
    "#            None\n",
    "\n",
    "#     self.image_data = []  # Use this list to store all image data\n",
    "#     label_dict = {'training': 'validation', 'validation': 'test', 'test': ''}\n",
    "\n",
    "#     with open(path, mode='r') as file:\n",
    "#       metadata = csv.reader(file)\n",
    "#       next(metadata)  # Skip the header\n",
    "#       for row in metadata:\n",
    "#         if row[0] == training_type:\n",
    "#           self.image_data.append((row[1], row[2], self.root + row[1] + \".jpg\"))\n",
    "#         elif row[0] == label_dict[training_type]:\n",
    "#           break  # Stop when we reach the end of the current training type section\n",
    "        \n",
    "#     if training_type == 'training' and downsample_nc:\n",
    "#       self.downsample_nc()\n",
    "            \n",
    "#   def downsample_nc(self):\n",
    "#     # Separate 'nc' class data and other data\n",
    "#     nc_data = [item for item in self.image_data if item[1] == 'nc']\n",
    "#     other_data = [item for item in self.image_data if item[1] != 'nc']\n",
    "\n",
    "#     # Downsample 'nc' data to 50% of its original size\n",
    "#     reduced_nc_data = random.sample(nc_data, len(nc_data) // 2)\n",
    "\n",
    "#     # Combine and shuffle\n",
    "#     self.image_data = other_data + reduced_nc_data\n",
    "#     random.shuffle(self.image_data)\n",
    "\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.image_data) \n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#     image_id, classification, image_path = self.image_data[idx]\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image = self.transform(image)\n",
    "    \n",
    "#     if self.preprocess:\n",
    "#         # Apply preprocess transformations (normalization, etc.)\n",
    "#         # Note that you might need to move and reshape the image tensor for normalization.\n",
    "#         mean = torch.mean(image, dim=(1, 2))\n",
    "#         std = torch.std(image, dim=(1, 2))\n",
    "#         preprocess_transform = transforms.Compose([\n",
    "#             transforms.Normalize(mean=mean.tolist(), std=std.tolist()),\n",
    "#             transforms.GaussianBlur(kernel_size=3, sigma=(0.1))\n",
    "#         ])\n",
    "#         image = preprocess_transform(image)\n",
    "\n",
    "#     return image_id, classification, image\n",
    "\n",
    "\n",
    "# training_data = ImageData(preprocess=True, dataset_type='complete', training_type='training', downsample_nc=True)\n",
    "# validation_data = ImageData(preprocess=True, dataset_type='complete', training_type='validation')\n",
    "# test_data = ImageData(preprocess=True, dataset_type='complete', training_type='test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative model: LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size=(224, 224)):\n",
    "        super(LeNetBinaryClassifier, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Calculate the size of the output from the last conv/pooling block.\n",
    "        def conv_output_size(img_size, padding, kernel_size, stride):\n",
    "            return (img_size + 2*padding - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        # Calculations assuming the input image is a square of size initial_size x initial_size\n",
    "        size = conv_output_size(input_size[0], 0, 5, 1)  # conv1: (224-5)/1 + 1 = 220\n",
    "        size = conv_output_size(size, 0, 2, 2)           # pool1: (220-2)/2 + 1 = 110\n",
    "        size = conv_output_size(size, 0, 5, 1)           # conv2: (110-5)/1 + 1 = 106\n",
    "        size = conv_output_size(size, 0, 2, 2)           # pool2: (106-2)/2 + 1 = 53\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * size * size, 120)      # Adjust the input size\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHvE0EB6rvEx",
    "outputId": "8ebc9dc2-9fbc-490c-bb61-97c799609528"
   },
   "outputs": [],
   "source": [
    "# # TODO: Adjust input dimensions\n",
    "\n",
    "# model1 = LeNetBinaryClassifier(input_size=(224, 224)).to(device)\n",
    "model1 = FirstModel().to(device)\n",
    "\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "\n",
    "training = ImageData(preprocess=Preprocess, dataset_type='complete', training_type='training')\n",
    "training_data = DataLoader(dataset=training, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "validation = ImageData(preprocess=Preprocess, dataset_type='complete', training_type='validation')\n",
    "validation_data = DataLoader(dataset=validation, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "lr = .001\n",
    "num_epochs = 3\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=lr)\n",
    "\n",
    "# Train:\n",
    "\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "# Train:\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "\n",
    "    for batch in tqdm(training_data):\n",
    "        _, labels, images = batch\n",
    "        num_labels = torch.tensor([1 if (label == 'mel' or label == 'bcc') else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model1(images)\n",
    "        loss = criterion(outputs, num_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = (outputs > 0).float()\n",
    "        total += num_labels.size(0)\n",
    "        correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "        \n",
    "    # Store the average training accuracy for the current epoch\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch:{epoch}, training accuracy: {train_accuracy}\")\n",
    "    training_accuracies.append(train_accuracy)\n",
    "    \n",
    "    model1.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in validation_data:\n",
    "            _, labels, images = batch\n",
    "\n",
    "            num_labels = torch.tensor([1 if (label == 'mel' or label == 'bcc') else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "    #         print(images.shape) \n",
    "            outputs = model1(images)\n",
    "            predicted = (outputs > 0).float()\n",
    "\n",
    "            total += num_labels.size(0)\n",
    "            correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        validation_accuracies.append(val_accuracy)\n",
    "        print(f\"Correct: {correct}; Total: {total}\")\n",
    "        print(f'Validation Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x=range(1, num_epochs+1), y=training_accuracies)\n",
    "plt.title('Training Accuracy over Epochs for LeNet')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy (%)')\n",
    "plt.savefig('model1_train_acc_original_lenet.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x=range(1, num_epochs+1), y=validation_accuracies)\n",
    "plt.title('Validation Accuracy over Epochs for LeNet')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_2_with_residual().to(device)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# num_epochs = 5\n",
    "\n",
    "# # Lists for tracking accuracy\n",
    "# training_accuracies = []\n",
    "# validation_accuracies = []\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training phase\n",
    "#     model.train()\n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "\n",
    "#     for batch in tqdm(training_data, desc='Training Epoch {}'.format(epoch+1)):\n",
    "#         _, labels, images = batch\n",
    "#         images = images.to(device)\n",
    "#         # Convert labels to a tensor of 0s and 1s\n",
    "#         num_labels = torch.tensor([1 if label == 'mel' or label == 'bcc' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, num_labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         predicted = torch.round(torch.sigmoid(outputs))\n",
    "#         train_total += num_labels.size(0)\n",
    "#         train_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "#     train_accuracy = 100 * train_correct / train_total\n",
    "#     training_accuracies.append(train_accuracy)\n",
    "    \n",
    "#     # Validation phase\n",
    "#     model.eval()\n",
    "#     valid_correct = 0\n",
    "#     valid_total = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(validation_data, desc='Validation Epoch {}'.format(epoch+1)):\n",
    "#             _, labels, images = batch\n",
    "#             images = images.to(device)\n",
    "#             num_labels = torch.tensor([1 if label == 'mel' or label == 'bcc' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "            \n",
    "#             outputs = model(images)\n",
    "#             predicted = torch.round(torch.sigmoid(outputs))\n",
    "#             valid_total += num_labels.size(0)\n",
    "#             valid_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "#     valid_accuracy = 100 * valid_correct / valid_total\n",
    "#     validation_accuracies.append(valid_accuracy)\n",
    "    \n",
    "#     # Print accuracies for the epoch\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {valid_accuracy:.2f}%')\n",
    "\n",
    "# # Plotting the training and validation accuracies\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.lineplot(range(1, num_epochs + 1), training_accuracies, marker='o', label='Training Accuracy')\n",
    "# sns.lineplot(range(1, num_epochs + 1), validation_accuracies, marker='o', label='Validation Accuracy')\n",
    "# plt.title('Training and Validation Accuracies')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x=range(1, num_epochs+1), y=training_accuracies)\n",
    "plt.title('Training Accuracy over Epochs for the First Model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy (%)')\n",
    "plt.savefig('model1_train_acc_downsampled_our_model.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class breakdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def check_class_distribution(data_loader):\n",
    "    label_counter = Counter()\n",
    "    for _, labels, _ in data_loader:\n",
    "        label_counter.update(labels)\n",
    "    return label_counter\n",
    "\n",
    "# Use check_class_distribution on your training_data and validation_data\n",
    "train_class_dist = check_class_distribution(training_data)\n",
    "val_class_dist = check_class_distribution(validation_data)\n",
    "\n",
    "print(\"Training set class distribution:\", train_class_dist)\n",
    "print(\"Validation set class distribution:\", val_class_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# # Lists to store all true labels and predicted labels for the validation dataset\n",
    "# all_true_labels = []\n",
    "# all_pred_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in validation_data:\n",
    "#         _, labels, images = batch\n",
    "#         num_labels = torch.tensor([1 if (label == 'mel' or label == 'bcc') else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "#         outputs = model(images)\n",
    "        \n",
    "#         # Since outputs are logits, apply sigmoid and then threshold at 0.5 for binary classification\n",
    "#         preds = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "#         # Move predictions and true labels to CPU and convert them to NumPy arrays\n",
    "#         preds = preds.cpu().numpy()\n",
    "#         labels_np = num_labels.cpu().numpy()\n",
    "        \n",
    "#         # Extend the lists with the flattened arrays\n",
    "#         all_true_labels.extend(labels_np.flatten())\n",
    "#         all_pred_labels.extend(preds.flatten())\n",
    "\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "# print(f'Validation F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mn2I3rgM8b8"
   },
   "source": [
    "# **The second model:**\n",
    "- Used if the first model which cancer (bcc or mel)\n",
    "- Predict which type of cancer it is.\n",
    "\n",
    "Architecture:\n",
    "1. Input layer: 224, 224, 3\n",
    "\n",
    "2. Convolutional blocks: (mayebe 3)\n",
    "  - Block 1:  \n",
    "    - filters=32, kernel_size=(3, 3), activation='relu', padding='same' - activation and kernel size are subject to change\n",
    "    - BatchNormalization(),\n",
    "    - MaxPooling2D(pool_size=(2, 2)) - subject to change\n",
    "    - Dropout(0.1) - dropout rate is subject to change\n",
    "    \n",
    "  - Block 2:\n",
    "    - filters=64, kernel_size=(3, 3), activation='relu', padding='same' - activation and kernel size are subject to change\n",
    "    - BatchNormalization(),\n",
    "    - MaxPooling2D(pool_size=(2, 2)),\n",
    "    - Dropout(0.25) - dropout rate is subject to change\n",
    "\n",
    "  - Block 3 (last block):\n",
    "    - Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    - BatchNormalization(),\n",
    "    - MaxPooling2D(pool_size=(2, 2)),\n",
    "    - Dropout(0.25),\n",
    "\n",
    "3. Fully connected layer (after flatten the output from convolutional layer):\n",
    "  - Dense(256, activation='relu'),\n",
    "  - Dropout(0.2)\n",
    "\n",
    "4. Output:\n",
    "  - Dense(num_classes, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgVCHm-fAHDw"
   },
   "source": [
    "<!-- class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "  \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return x -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "723_Sgc0Xo89"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Tunable Parameters:\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "\n",
    "training = ImageData(preprocess=Preprocess, dataset_type='cancerous', training_type='training')\n",
    "training_data = DataLoader(dataset=training, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "validation = ImageData(preprocess=Preprocess, dataset_type='cancerous', training_type='validation')\n",
    "validation_data = DataLoader(dataset=validation, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "test = ImageData(preprocess=Preprocess, dataset_type='cancerous', training_type='test')\n",
    "test_data = DataLoader(dataset=test, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmXV68d8pmxp"
   },
   "source": [
    "# Model 2 without residual connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7-pwhFDODn4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model2, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "# add residual connection\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(100352, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return torch.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFufz0t0AfTX"
   },
   "source": [
    "# Model2 architecture with residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNsWbN4wv-4f"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class model_2_with_residual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_2_with_residual, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Adjustment for channel increase and potential size difference in residual connection\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 1, stride=2, bias=False),  # Stride set to 2 for spatial size reduction\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.residual_block = ResidualBlock(64, 128, stride=2, downsample=self.downsample)\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1),  # Adjusted input channels to 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(25088, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        # You will need to recalculate the size here based on your network's architecture\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 1)\n",
    "#         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.residual_block(out)  # Applying the residual block\n",
    "        out = self.block3(out)         # Additional layers after the residual block\n",
    "\n",
    "        out = out.view(out.size(0), -1)  # Flatten the output for the fully connected layer\n",
    "        out = self.fc(out)               # Fully connected layer operation\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kqj0Dvt62sQd"
   },
   "source": [
    "# Train and validate model 2 With Residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_2_with_residual().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Lists for tracking accuracy\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch in tqdm(training_data, desc='Training Epoch {}'.format(epoch+1)):\n",
    "        _, labels, images = batch\n",
    "        images = images.to(device)\n",
    "        # Convert labels to a tensor of 0s and 1s\n",
    "        num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, num_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        train_total += num_labels.size(0)\n",
    "        train_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    training_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data, desc='Validation Epoch {}'.format(epoch+1)):\n",
    "            _, labels, images = batch\n",
    "            images = images.to(device)\n",
    "            num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            valid_total += num_labels.size(0)\n",
    "            valid_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    validation_accuracies.append(valid_accuracy)\n",
    "    \n",
    "    # Print accuracies for the epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {valid_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(range(1, num_epochs + 1), training_accuracies, marker='o', label='Training Accuracy')\n",
    "sns.lineplot(range(1, num_epochs + 1), validation_accuracies, marker='o', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracies of Model 2 With Residual Connection')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.savefig('train_valid_acc_model_2_residual.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-as9Vgh3BKv"
   },
   "source": [
    "# Train and validate model 2 without residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VvhDiJnhFUy"
   },
   "outputs": [],
   "source": [
    "model = model2().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Lists for tracking accuracy\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch in tqdm(training_data, desc='Training Epoch {}'.format(epoch+1)):\n",
    "        _, labels, images = batch\n",
    "        images = images.to(device)\n",
    "        # Convert labels to a tensor of 0s and 1s\n",
    "        num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, num_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        train_total += num_labels.size(0)\n",
    "        train_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    training_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data, desc='Validation Epoch {}'.format(epoch+1)):\n",
    "            _, labels, images = batch\n",
    "            images = images.to(device)\n",
    "            num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            valid_total += num_labels.size(0)\n",
    "            valid_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    validation_accuracies.append(valid_accuracy)\n",
    "    \n",
    "    # Print accuracies for the epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {valid_accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# def check_class_distribution(data_loader):\n",
    "#     label_counter = Counter()\n",
    "#     for _, labels, _ in data_loader:\n",
    "#         label_counter.update(labels)\n",
    "#     return label_counter\n",
    "\n",
    "# # Use check_class_distribution on your training_data and validation_data\n",
    "# train_class_dist = check_class_distribution(training_data)\n",
    "# val_class_dist = check_class_distribution(validation_data)\n",
    "# test_class_dist = check_class_distribution(test_data)\n",
    "\n",
    "# print(\"Training set class distribution:\", train_class_dist)\n",
    "# print(\"Validation set class distribution:\", val_class_dist)\n",
    "# print(\"Test set class distribution:\", test_class_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(range(1, num_epochs + 1), training_accuracies, marker='o', label='Training Accuracy')\n",
    "sns.lineplot(range(1, num_epochs + 1), validation_accuracies, marker='o', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracies of Model 2 Without Residual Connection')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.savefig('train_valid_acc_model_2_no_residual.png')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.lineplot(range(1, num_epochs + 1), validation_accuracies, marker='o', label='Validation Accuracy')\n",
    "# plt.title('Validation Accuracy of Model 2 Without Residual Connection')\n",
    "\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.legend()\n",
    "# plt.savefig('valid_acc_model_2.png')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_data):\n",
    "        _, labels, images = batch\n",
    "        images = images.to(device)\n",
    "        num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        test_total += num_labels.size(0)\n",
    "        test_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "# Print accuracies for the epoch\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_2_with_residual().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Lists for tracking accuracy\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch in tqdm(training_data, desc='Training Epoch {}'.format(epoch+1)):\n",
    "        _, labels, images = batch\n",
    "        images = images.to(device)\n",
    "        # Convert labels to a tensor of 0s and 1s\n",
    "        num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, num_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        train_total += num_labels.size(0)\n",
    "        train_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    training_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data, desc='Validation Epoch {}'.format(epoch+1)):\n",
    "            _, labels, images = batch\n",
    "            images = images.to(device)\n",
    "            num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            valid_total += num_labels.size(0)\n",
    "            valid_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    validation_accuracies.append(valid_accuracy)\n",
    "    \n",
    "    # Print accuracies for the epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {valid_accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation accuracies\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(range(1, num_epochs + 1), training_accuracies, marker='o', label='Training Accuracy')\n",
    "sns.lineplot(range(1, num_epochs + 1), validation_accuracies, marker='o', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracies of Model 2 With Residual Connection, on Preprocessed Images')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.savefig('train_valid_acc_model_2_residual_preprocessed.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_data):\n",
    "        _, labels, images = batch\n",
    "        images = images.to(device)\n",
    "        num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        test_total += num_labels.size(0)\n",
    "        test_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "# Print accuracies for the epoch\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model2().to(device)\n",
    "count_mel = 890\n",
    "count_bcc = 408\n",
    "\n",
    "# Total count\n",
    "total_count = count_mel + count_bcc\n",
    "\n",
    "# Calculate class weights for 'mel' and 'bcc'\n",
    "weight_mel = total_count / (2 * count_mel)\n",
    "weight_bcc = total_count / (2 * count_bcc)\n",
    "\n",
    "# Use the higher weight for the less represented class\n",
    "# Assuming 'bcc' is the class of interest (the positive class)\n",
    "class_weight = weight_bcc\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "class_weight_tensor = torch.tensor(class_weight, dtype=torch.float).to(device)\n",
    "\n",
    "# Use this weight on your BCEWithLogitsLoss\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weight_tensor)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Lists for tracking accuracy\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch in tqdm(training_data, desc='Training Epoch {}'.format(epoch+1)):\n",
    "        _, labels, images = batch\n",
    "        images = images.to(device)\n",
    "        # Convert labels to a tensor of 0s and 1s\n",
    "        num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, num_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        train_total += num_labels.size(0)\n",
    "        train_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    training_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data, desc='Validation Epoch {}'.format(epoch+1)):\n",
    "            _, labels, images = batch\n",
    "            images = images.to(device)\n",
    "            num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            valid_total += num_labels.size(0)\n",
    "            valid_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    validation_accuracies.append(valid_accuracy)\n",
    "    \n",
    "    # Print accuracies for the epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {valid_accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation accuracies\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(range(1, num_epochs + 1), training_accuracies, marker='o', label='Training Accuracy')\n",
    "sns.lineplot(range(1, num_epochs + 1), validation_accuracies, marker='o', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracies of Model 2 Without Residual Connection, on Preprocessed Images')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.savefig('train_valid_acc_model_2_no_residual_preprocessed.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_data):\n",
    "        _, labels, images = batch\n",
    "        images = images.to(device)\n",
    "        num_labels = torch.tensor([1 if label == 'mel' else 0 for label in labels], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        test_total += num_labels.size(0)\n",
    "        test_correct += (predicted == num_labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "# Print accuracies for the epoch\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2bNXaP-7lKJ"
   },
   "source": [
    "# **The third model:**\n",
    "\n",
    "*   Used if the first model outputs non-cancerous\n",
    "*   Classifies which non-cancerous skin lesion\n",
    "\n",
    "We can test on existing models and our own and contrast ????\n",
    "Break data down into:\n",
    "- 80% training\n",
    "- 10% validation\n",
    "- 10% testing\n",
    "\n",
    "Small dataset\n",
    "\n",
    "Hyper-parameters (To be Tuned):\n",
    "- Batch Size: 32\n",
    "- Learning Rate: 1e-4\n",
    "- Decay Factor / L2 Reg: 0.0001\n",
    "- Optimizer: Adam (momentum for GD)\n",
    "- Epochs: 20\n",
    "\n",
    "Architecture:\n",
    "1. Pre-process images:\n",
    "    - rescale to 224x224x3\n",
    "    - normalize to center around 0 (divide each pixel by 255)\n",
    "    - smooth it out\n",
    "\n",
    "2. Input Image: 224x224x3\n",
    "\n",
    "3. Convolutional blocks: (5 of them)\n",
    "  - Block 1:  \n",
    "    - filters=560, kernel_size=(3, 3), activation='relu', padding='same' - activation and kernel size are subject to change\n",
    "    - BatchNormalization(),\n",
    "    - MaxPooling2D(pool_size=(2, 2)) - subject to change\n",
    "    - Dropout(0.1) - dropout rate is subject to change\n",
    "    \n",
    "  - Block 2:\n",
    "    - filters=280, kernel_size=(3, 3), activation='relu', padding='same' - activation and kernel size are subject to change\n",
    "    - BatchNormalization(),\n",
    "    - MaxPooling2D(pool_size=(2, 2)),\n",
    "    - Dropout(0.25) - dropout rate is subject to change\n",
    "\n",
    "  - Block 3:\n",
    "    - Conv2D(filters=140, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    - BatchNormalization(),\n",
    "    - MaxPooling2D(pool_size=(2, 2)),\n",
    "    - Dropout(0.25),\n",
    "  - Block 4:\n",
    "    - Conv2D(filters=70, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    - BatchNormalization(),\n",
    "    - MaxPooling2D(pool_size=(2, 2)),\n",
    "    - Dropout(0.25),\n",
    "  - Block 5:\n",
    "    - Conv2D(filters=35, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    - BatchNormalization(),\n",
    "    - MaxPooling2D(pool_size=(2, 2)),\n",
    "    - Dropout(0.25),\n",
    "\n",
    "3. Fully connected layer (after flatten the output from convolutional layer):\n",
    "  - Dense(256, activation='relu'),\n",
    "  - Dropout(0.2)\n",
    "\n",
    "4. Output:\n",
    "  - Dense(num_classes, activation='softmax')\n",
    "\n",
    "Things to calculate:\n",
    "- Accuracy: TP + TN / TP + TN + FP + FN\n",
    "- Recall: TP / TP + TN\n",
    "- Precision: TP / TP + FP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edPwQKPjNSEh"
   },
   "source": [
    "## Set Up Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0q4dsVhNVU2"
   },
   "outputs": [],
   "source": [
    "# Tunable Parameters:\n",
    "batch_size = 128\n",
    "learning_rate, weight_decay, num_epochs = 0.001, 0.01, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B7wswlNNKso"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLUGtGItNMvZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "shuffle = True\n",
    "\n",
    "training = ImageData(preprocess=Preprocess, dataset_type='non_cancerous', training_type='training')\n",
    "training_data = DataLoader(dataset=training, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "validation = ImageData(preprocess=Preprocess, dataset_type='non_cancerous', training_type='validation')\n",
    "validation_data = DataLoader(dataset=validation, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "test = ImageData(preprocess=Preprocess, dataset_type='non_cancerous', training_type='test')\n",
    "test_data = DataLoader(dataset=test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "nc_labels = ['akiec', 'bkl', 'df', 'nv', 'vasc']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eANv6_7FAg6x"
   },
   "source": [
    "## Model 3 without Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vo5OM_SV21WU"
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model3, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # should be  32x112x112\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # should be 64x56x56\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "\n",
    "        # 128x28x28\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(100352, 256), #18432\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        # x = self.block4(x)\n",
    "        # x = self.block5(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wbibS9o_BT3"
   },
   "source": [
    "# Model 3 with residual connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz_0JPBa_Eps"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock_model3(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock_model3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class model3_residual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model3_residual, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # should be  32x112x112\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.residual_block = ResidualBlock_model3(64, 128, stride=2, downsample=self.downsample)\n",
    "\n",
    "        # Assuming block3 remains the same as before\n",
    "        self.block3 = nn.Sequential(\n",
    "          nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "          nn.Dropout(0.2)\n",
    "      )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(25088, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.residual_block(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.size())\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYRhkPLxeYav"
   },
   "source": [
    "\n",
    "# Evaluate function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pgMKO-eya6A"
   },
   "outputs": [],
   "source": [
    "# def evaluate(model, loader):  # Evaluate accuracy on validation set\n",
    "#     print(len(loader.dataset))\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(loader):\n",
    "#             _, labels, images = batch\n",
    "#             # batch = batch.to(device)\n",
    "#             # label = label.to(device)\n",
    "#             pred = model(images)\n",
    "#             # print(labels)\n",
    "#             # label = labels[0]\n",
    "#             for label, prediction in zip(labels, pred):\n",
    "#               correct += (nc_labels[torch.argmax(prediction)] == label)\n",
    "#         acc = correct/len(loader.dataset)\n",
    "#         print(\"\\n Evaluation accuracy: {}\".format(acc))\n",
    "#         return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nveS0At2_ltC"
   },
   "source": [
    "# Train model 3 without residual connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xw8DWf4meX8s",
    "outputId": "788a0bf3-0415-4c94-9ae5-09fb90f88071"
   },
   "outputs": [],
   "source": [
    "# Train:\n",
    "print(\"Start training...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model3().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "trn_loss_hist = []\n",
    "trn_acc_hist = []\n",
    "val_loss_hist = []  # Initialize validation loss history\n",
    "val_acc_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    total_correct = 0  # Initialize total correct predictions\n",
    "    total_samples = 0  # Initialize total samples\n",
    "\n",
    "    print('-----------------Epoch = %d-----------------' % (epoch+1))\n",
    "\n",
    "    for batch in tqdm(training_data):\n",
    "        _, labels, images = batch\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(images)\n",
    "\n",
    "        label_index = [nc_labels.index(label) for label in labels]  # Convert labels to indices\n",
    "        label_tensor = torch.tensor(label_index, dtype=torch.long).to(device)\n",
    "        loss = criterion(pred, label_tensor)\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        total_correct += (predicted == label_tensor).sum().item()\n",
    "        total_samples += label_tensor.size(0)\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    epoch_loss = np.mean(running_loss)\n",
    "    trn_loss_hist.append(epoch_loss)\n",
    "\n",
    "    # Calculate accuracy for the epoch\n",
    "    epoch_acc = 100 * total_correct / total_samples\n",
    "    trn_acc_hist.append(epoch_acc)\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(\"\\n Epoch {} loss:{:.4f}, Accuracy: {:.2f}%\".format(epoch+1, epoch_loss, epoch_acc))\n",
    "\n",
    "    # Validation phase (similar to the previous example)\n",
    "    print(\"\\n Evaluate on validation set...\")\n",
    "    val_loss = 0.0  # Initialize validation loss accumulator\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data):\n",
    "            _, labels, images = batch\n",
    "            images = images.to(device)\n",
    "            label_indices = [nc_labels.index(label) for label in labels]\n",
    "            label_tensor = torch.tensor(label_indices, dtype=torch.long).to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, label_tensor)\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += label_tensor.size(0)\n",
    "            val_correct += (predicted == label_tensor).sum().item()\n",
    "\n",
    "    val_loss /= len(validation_data)  # Calculate average validation loss\n",
    "    val_loss_hist.append(val_loss)  # Store average validation loss\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    val_acc_hist.append(val_acc)\n",
    "\n",
    "    # Print validation statistics\n",
    "    print(f'Validation Loss: {val_loss:.4f})\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOYVMxCgn-bO"
   },
   "source": [
    "## PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "UbyZKEIUn8xc",
    "outputId": "46d04514-0f52-455f-b80f-79d241500b67"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), val_loss_hist, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Cross-Entropy Loss (Validation)')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), trn_acc_hist, label='Training')\n",
    "# plt.plot(range(1, num_epochs + 1), val_acc_hist, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.ylim(70, 85)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mf1ztWGn_vg6"
   },
   "source": [
    "# Train model 3 with residual connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgvcEgpKdi4R",
    "outputId": "295ac7f2-2f18-4289-b126-57ae1a420176"
   },
   "outputs": [],
   "source": [
    "# model3_residual = model3_residual().to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# trn_loss_hist = []\n",
    "# trn_acc_hist = []\n",
    "# val_loss_hist = []\n",
    "# val_acc_hist = []\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model3_residual.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for i, batch in enumerate(tqdm(training_data)):\n",
    "#         _, labels, images = batch\n",
    "#         images = images.to(device)\n",
    "#         label_indices = [nc_labels.index(label) for label in labels]\n",
    "#         label_tensor = torch.tensor(label_indices, dtype=torch.long).to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model3_residual(images)\n",
    "#         loss = criterion(outputs, label_tensor)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         total += label_tensor.size(0)\n",
    "#         correct += (predicted == label_tensor).sum().item()\n",
    "\n",
    "#         # Print minibatch loss every 5 minibatches\n",
    "# #         if i % 5 == 0:\n",
    "# #             print('[Epoch %d, Minibatch %5d] loss: %.4f' %\n",
    "# #                   (epoch + 1, i + 1, running_loss / 5))\n",
    "# #             running_loss = 0.0\n",
    "\n",
    "#     # Calculate average loss and accuracy for training data\n",
    "#     avg_loss = running_loss / len(training_data)\n",
    "#     trn_loss_hist.append(avg_loss)\n",
    "#     trn_acc = 100 * correct / total\n",
    "#     trn_acc_hist.append(trn_acc)\n",
    "\n",
    "#     # Evaluate on validation data\n",
    "#     model3_residual.eval()\n",
    "#     val_running_loss = 0.0\n",
    "#     val_correct = 0\n",
    "#     val_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(validation_data):\n",
    "#             _, labels, images = batch\n",
    "#             images = images.to(device)\n",
    "#             label_indices = [nc_labels.index(label) for label in labels]\n",
    "#             label_tensor = torch.tensor(label_indices, dtype=torch.long).to(device)\n",
    "#             outputs = model3_residual(images)\n",
    "#             loss = criterion(outputs, label_tensor)\n",
    "#             val_running_loss += loss.item()\n",
    "\n",
    "#             # Calculate accuracy\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             val_total += label_tensor.size(0)\n",
    "#             val_correct += (predicted == label_tensor).sum().item()\n",
    "\n",
    "#     # Calculate average loss and accuracy for validation data\n",
    "#     val_avg_loss = val_running_loss / len(validation_data)\n",
    "#     val_loss_hist.append(val_avg_loss)\n",
    "#     val_acc = 100 * val_correct / val_total\n",
    "#     val_acc_hist.append(val_acc)\n",
    "\n",
    "#     # Print epoch statistics\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {trn_acc:.2f}%')\n",
    "#     print(f'Validation Loss: {val_avg_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train:\n",
    "print(\"Start training...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model3_residual = model3_residual().to(device)\n",
    "optimizer = torch.optim.Adam(model3_residual.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "trn_loss_hist = []\n",
    "trn_acc_hist = []\n",
    "val_loss_hist = []  # Initialize validation loss history\n",
    "val_acc_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model3_residual.train()\n",
    "    running_loss = []\n",
    "    total_correct = 0  # Initialize total correct predictions\n",
    "    total_samples = 0  # Initialize total samples\n",
    "\n",
    "    print('-----------------Epoch = %d-----------------' % (epoch+1))\n",
    "\n",
    "    for batch in tqdm(training_data):\n",
    "        _, labels, images = batch\n",
    "        optimizer.zero_grad()\n",
    "        pred = model3_residual(images)\n",
    "\n",
    "        label_index = [nc_labels.index(label) for label in labels]  # Convert labels to indices\n",
    "        label_tensor = torch.tensor(label_index, dtype=torch.long).to(device)\n",
    "        loss = criterion(pred, label_tensor)\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        total_correct += (predicted == label_tensor).sum().item()\n",
    "        total_samples += label_tensor.size(0)\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    epoch_loss = np.mean(running_loss)\n",
    "    trn_loss_hist.append(epoch_loss)\n",
    "\n",
    "    # Calculate accuracy for the epoch\n",
    "    epoch_acc = 100 * total_correct / total_samples\n",
    "    trn_acc_hist.append(epoch_acc)\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(\"\\n Epoch {} loss:{:.4f}, Accuracy: {:.2f}%\".format(epoch+1, epoch_loss, epoch_acc))\n",
    "\n",
    "    # Validation phase (similar to the previous example)\n",
    "    print(\"\\n Evaluate on validation set...\")\n",
    "    val_loss = 0.0  # Initialize validation loss accumulator\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data):\n",
    "            _, labels, images = batch\n",
    "            images = images.to(device)\n",
    "            label_indices = [nc_labels.index(label) for label in labels]\n",
    "            label_tensor = torch.tensor(label_indices, dtype=torch.long).to(device)\n",
    "            outputs = model3_residual(images)\n",
    "            loss = criterion(outputs, label_tensor)\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += label_tensor.size(0)\n",
    "            val_correct += (predicted == label_tensor).sum().item()\n",
    "\n",
    "    val_loss /= len(validation_data)  # Calculate average validation loss\n",
    "    val_loss_hist.append(val_loss)  # Store average validation loss\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    val_acc_hist.append(val_acc)\n",
    "\n",
    "    # Print validation statistics\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "mI0jJG_TfJqL",
    "outputId": "f6bd59fa-f1c7-4ede-af14-d4da1df0ebd8"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), val_loss_hist, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Cross-Entropy Loss (Validation)')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), trn_acc_hist, label='Training')\n",
    "plt.plot(range(1, num_epochs + 1), val_acc_hist, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.ylim(0, 85)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEgrJiEphNII"
   },
   "source": [
    "# **Test Evaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZMooHtIhRzF",
    "outputId": "8f6d0e3b-3b5d-4036-a5d6-d32779e84642"
   },
   "outputs": [],
   "source": [
    "evaluate(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medicine-chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def consult_ai(age, sex, location, diagnosis):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"AdaptLLM/medicine-chat\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"AdaptLLM/medicine-chat\")\n",
    "    \n",
    "    user_input = f\"\"\"I am a patient with {diagnosis} disease on my {location} area of my body.\n",
    "    I am {age} years old and my sex is {sex}. Apart from immediately consulting a doctor,\n",
    "    what should my next steps be?\"\"\"\n",
    "    \n",
    "    our_system_prompt = \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\" # Please do NOT change this\n",
    "    prompt = f\"<s>[INST] <<SYS>>{our_system_prompt}<</SYS>>\\n\\n{user_input} [/INST]\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "    outputs = model.generate(input_ids=inputs, max_length=4096)[0]\n",
    "    \n",
    "    answer_start = int(inputs.shape[-1])\n",
    "    pred = tokenizer.decode(outputs[answer_start:], skip_special_tokens=True)\n",
    "\n",
    "    print(pred)\n",
    "\n",
    "consult_ai(18, \"male\", \"behind ear\", \"Melanoma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"AdaptLLM/medicine-chat\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"AdaptLLM/medicine-chat\")\n",
    "\n",
    "def consult_ai(age, sex, location, diagnosis):\n",
    "    user_input = f\"\"\"I am a patient with {diagnosis} disease on my {location} area of my body.\n",
    "    I am {age} years old and my sex is {sex}. Apart from immediately consulting a doctor,\n",
    "    what should my next steps be?\"\"\"\n",
    "    \n",
    "    our_system_prompt = \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\" # Please do NOT change this\n",
    "    prompt = f\"<s>[INST] <<SYS>>{our_system_prompt}<</SYS>>\\n\\n{user_input} [/INST]\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=200)[0]\n",
    "\n",
    "    answer_start = int(inputs.shape[-1])\n",
    "    pred = tokenizer.decode(outputs[answer_start:], skip_special_tokens=True)\n",
    "    \n",
    "    print(pred)\n",
    "\n",
    "consult_ai(65, \"female\", \"on nose\", \"Bowen's Disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "def consult_gemini(age, sex, location, diagnosis):\n",
    "    user_input = f\"\"\"I am a patient with {diagnosis} disease on my {location} area of my body.\n",
    "    I am {age} years old and my sex is {sex}. Apart from immediately consulting a doctor,\n",
    "    what should my next steps be?\"\"\"\n",
    "    \n",
    "    genai.configure(api_key='ak') # Real API Key is hiddeb deliberately\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    response = model.generate_content(user_input)\n",
    "    \n",
    "    print(response.text)\n",
    "\n",
    "consult_gemini(65, \"female\", \"on nose\", \"melanoma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver(modelOne, modelTwo image_path, preprocess, labels):\n",
    "    try:\n",
    "      image = Image.open(image_path).convert(\"RGB\")\n",
    "    except:\n",
    "      print(\"Image not found\")\n",
    "\n",
    "    if preprocess:\n",
    "      mean = torch.mean(image, dim=(1, 2))\n",
    "      std = torch.std(image, dim=(1, 2))\n",
    "      preprocess_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1))\n",
    "      ])\n",
    "      image = preprocess_transform(image)\n",
    "      outputs=modelOne(image)\n",
    "\n",
    "      pred_prob = torch.sigmoid(outputs)\n",
    "      runModel1 = false\n",
    "\n",
    "      print(\"the given skin abnormality is\")\n",
    "      if pred_prob[0] > .8 :\n",
    "        print(\"very likely malignant\")\n",
    "        runModel = true\n",
    "      elif pred_prob[0] > .6 :\n",
    "        print(\"likely malignant\")\n",
    "        runModel = true\n",
    "      elif pred_prob[0] > .4 :\n",
    "        print(\"neutral\")\n",
    "      elif pred_prob[0] > .2:\n",
    "        print(\"likely bengin\")\n",
    "      else :\n",
    "        print(\"very likely bengin\")\n",
    "\n",
    "      getInfo = \"\"\n",
    "      if(runModel1):\n",
    "        image = preprocess_transform(image)\n",
    "        outputs=modelOne(image)\n",
    "        pred_prob = torch.sigmoid(outputs)\n",
    "        print(\"probably of melanoma:\")\n",
    "        print(pred_prob)\n",
    "        if(pred_prob >= .5 ):\n",
    "          getInfo = \"mel\"\n",
    "        else:\n",
    "          getInfo = \"bcc\"\n",
    "\n",
    "      if(runModel1 == false ):\n",
    "        print(\"most likely output:\")\n",
    "        outputs = model(images)\n",
    "        predictedValue, predictedIndex = torch.max(outputs, 1)\n",
    "        print(labels[predictedIndex])\n",
    "        getInfo = labels[predictedIndex]\n",
    "        print(\"with probably:\")\n",
    "        print(predictedValue)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
